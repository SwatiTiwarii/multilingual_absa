{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import namedtuple\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking data set from Seeval 2016 - task 5 subset 1 : http://alt.qcri.org/semeval2016/task5/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from collections import Counter, defaultdict\n",
    "import re\n",
    "import ast\n",
    "pd.set_option('display.max_colwidth' , -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### English Laptop domain training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_multi_aspects = pd.read_csv('../data/English_laptops.csv')\n",
    "eng_multi_aspects['aspects'] = eng_multi_aspects['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "eng_multi_aspects['polarities'] = eng_multi_aspects['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "#eng_multi_aspects.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>aspects</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This computer is absolutely AMAZING!!!</td>\n",
       "      <td>[LAPTOP#GENERAL]</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10 plus hours of battery...</td>\n",
       "      <td>[BATTERY#OPERATION_PERFORMANCE]</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>super fast processor and really nice graphics card..</td>\n",
       "      <td>[CPU#OPERATION_PERFORMANCE, GRAPHICS#GENERAL]</td>\n",
       "      <td>[positive, positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and plenty of storage with 250 gb(though I will upgrade this and the ram..)</td>\n",
       "      <td>[HARD_DISC#DESIGN_FEATURES]</td>\n",
       "      <td>[positive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This computer is really fast and I'm shocked as to how easy it is to get used to...</td>\n",
       "      <td>[LAPTOP#OPERATION_PERFORMANCE, LAPTOP#USABILITY]</td>\n",
       "      <td>[positive, positive]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                  text  \\\n",
       "0  This computer is absolutely AMAZING!!!                                                \n",
       "1  10 plus hours of battery...                                                           \n",
       "2  super fast processor and really nice graphics card..                                  \n",
       "3  and plenty of storage with 250 gb(though I will upgrade this and the ram..)           \n",
       "4  This computer is really fast and I'm shocked as to how easy it is to get used to...   \n",
       "\n",
       "                                            aspects            polarities  \n",
       "0  [LAPTOP#GENERAL]                                  [positive]            \n",
       "1  [BATTERY#OPERATION_PERFORMANCE]                   [positive]            \n",
       "2  [CPU#OPERATION_PERFORMANCE, GRAPHICS#GENERAL]     [positive, positive]  \n",
       "3  [HARD_DISC#DESIGN_FEATURES]                       [positive]            \n",
       "4  [LAPTOP#OPERATION_PERFORMANCE, LAPTOP#USABILITY]  [positive, positive]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_multi_aspects[['text'  , 'aspects' , 'polarities']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/pandas/core/frame.py:3940: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  errors=errors)\n"
     ]
    }
   ],
   "source": [
    "eng_single_aspects = eng_multi_aspects[eng_multi_aspects['aspects'].apply(lambda x: len(x))==1]\n",
    "#eng_single_aspects['polarities']= eng_single_aspects['polarities'].apply(lambda x: x[0])\n",
    "#eng_single_aspects['aspects']= eng_single_aspects['aspects'].apply(lambda x: x[0])\n",
    "eng_single_aspects.drop(['aspects'] , axis = 1 , inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese data in Mobile and Camera domain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_single_aspects['polarities'] = eng_single_aspects['polarities'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>polarities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2031</th>\n",
       "      <td>do not use the facial recognition</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2032</th>\n",
       "      <td>Windows 8 is not supported by a lot of things!</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2034</th>\n",
       "      <td>It does what it advertises.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>I would recommend it, for anybody needing a reliable simple laptop.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2038</th>\n",
       "      <td>Love the price too!</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                     text  \\\n",
       "2031  do not use the facial recognition                                     \n",
       "2032  Windows 8 is not supported by a lot of things!                        \n",
       "2034  It does what it advertises.                                           \n",
       "2036  I would recommend it, for anybody needing a reliable simple laptop.   \n",
       "2038  Love the price too!                                                   \n",
       "\n",
       "     polarities  \n",
       "2031  negative   \n",
       "2032  negative   \n",
       "2034  positive   \n",
       "2036  positive   \n",
       "2038  positive   "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_single_aspects[['text', 'polarities']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "zu_camera_df = pd.read_csv('../data/Chinese_camera.csv')\n",
    "zu_camera_df['aspects'] = zu_camera_df['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "zu_camera_df['polarities'] = zu_camera_df['polarities'].apply(lambda x: ast.literal_eval(x))\n",
    "#zu_camera_df['polarities'] = zu_camera_df['polarities'].apply(lambda x: x[0])\n",
    "zu_camera_df.drop(['aspects'] , axis = 1 , inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zu_mobile_df = pd.read_csv('../data/Chinese_phones.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zu_mobile_df['aspects']  = zu_mobile_df['aspects'].apply(lambda x: ast.literal_eval(x))\n",
    "zu_mobile_df['polarities'] = zu_mobile_df['polarities'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zu_mobile_df['polarities'] = zu_mobile_df['polarities'].apply(lambda x : x[0])\n",
    "zu_mobile_df.drop(['aspects'] , axis = 1 , inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will extract embeddings for text column using https://github.com/facebookresearch/LASER . Saving text column separately for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"en\" ,\"nl\", 'es'\n",
    "eng_single_aspects[['text']].to_csv('../data/tatoeba/v1/en_laptop.txt' , header = None , index = None , mode = 'w')\n",
    "zu_camera_df[['text']].to_csv('../data/tatoeba/v1/zh_camera.txt' , header = None , index = None , mode = 'w')\n",
    "zu_mobile_df[['text']].to_csv('../data/tatoeba/v1/zh_mobile.txt' , header = None , index = None , mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASER_PATH = \"..\"\n",
    "sys.path.append(LASER_PATH + '/source')\n",
    "sys.path.append(LASER_PATH + '/source/lib')\n",
    "\n",
    "DATA_PATH = Path(\"../data/tatoeba/v1/\")\n",
    "CACHE_PATH = Path(\"cache/\")\n",
    "CACHE_PATH.mkdir(exist_ok=True)\n",
    "MODEL_PATH = Path(\"../models\")\n",
    "\n",
    "os.environ[\"LASER\"] = LASER_PATH \n",
    "SPACE_NORMALIZER = re.compile(\"\\s+\")\n",
    "Batch = namedtuple('Batch', 'srcs tokens lengths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from indexing import IndexLoad, IndexTextOpen, IndexTextQuery, IndexSearchKNN, IndexCreate, IndexSearchMultiple\n",
    "from embed import SentenceEncoder, EncodeLoad, EncodeFile\n",
    "from text_processing import Token, BPEfastApply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Following steps from https://medium.com/the-artificial-impostor/multilingual-similarity-search-using-pretrained-bidirectional-lstm-encoder-e34fac5958b0 for tokenization , BPE Fast and Embedding extractions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1284, 2), (1259, 2), (1333, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_single_aspects = eng_single_aspects[~eng_single_aspects['polarities'].apply(lambda x: 'neutral' in x)]\n",
    "eng_single_aspects.shape , zu_camera_df.shape , zu_mobile_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_single_aspects[['text']].to_csv('../data/tatoeba/v1/en_laptop.txt' , header = None , index = None , mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Tokenizer: en_laptop.txt in language en  \n",
      " - fast BPE: processing en_laptop.txt\n",
      " - Encoder: en_laptop.bpe to en_laptop.enc\n",
      " - Encoder: 1284 sentences in 0s\n",
      " - Tokenizer: zh_camera.txt exists already\n",
      " - Tokenizer: zh_mobile.txt exists already\n"
     ]
    }
   ],
   "source": [
    "#\"en\" ,\"nl\", 'es'\n",
    "\n",
    "encoder = SentenceEncoder(\n",
    "    str(MODEL_PATH / \"bilstm.93langs.2018-12-26.pt\"),\n",
    "    max_sentences=None,\n",
    "    max_tokens=10000,\n",
    "    cpu=False)\n",
    "bpe_codes = str(MODEL_PATH / \"93langs.fcodes\")\n",
    "\n",
    "for filename in (\"en_laptop\" ,\"zh_camera\", 'zh_mobile'):  ##\"zh\" for chinese , nl  for dutch and es for spanish\n",
    "    lang = filename[0:2]\n",
    "    Token(\n",
    "        str(DATA_PATH / f\"{filename}.txt\"), ##english_resturant.txt\n",
    "        str(CACHE_PATH / f\"{filename}.txt\"),\n",
    "        lang=lang,\n",
    "        romanize=False,\n",
    "        lower_case=True, gzip=False,\n",
    "        verbose=True)\n",
    "    BPEfastApply(\n",
    "        str(CACHE_PATH / f\"{filename}.txt\"),\n",
    "        str(CACHE_PATH / f\"{filename}.bpe\"),\n",
    "        bpe_codes,\n",
    "        verbose=True, over_write=True)\n",
    "    EncodeFile(\n",
    "        encoder,\n",
    "        str(CACHE_PATH / f\"{filename}.bpe\"),\n",
    "        str(CACHE_PATH / f\"{filename}.enc\"),\n",
    "        verbose=True, over_write=True)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Setence Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - embedding: cache/en_laptop.enc 1284 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/zh_camera.enc 1259 examples of dim 1024\n",
      " - creating FAISS index\n",
      " - embedding: cache/zh_mobile.enc 1333 examples of dim 1024\n",
      " - creating FAISS index\n"
     ]
    }
   ],
   "source": [
    "data_en, index_en = IndexCreate(\n",
    "    str(CACHE_PATH / \"en_laptop.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_zh_cam, index_zh_cam = IndexCreate(\n",
    "    str(CACHE_PATH / \"zh_camera.enc\"), 'FlatL2', verbose=True, save_index=False)\n",
    "data_zh_mob, index_zh_mob = IndexCreate(\n",
    "    str(CACHE_PATH / \"zh_mobile.enc\"), 'FlatL2', verbose=True, save_index=False)ding: cache/en_laptop.enc 1284 examples of dim 1024\n",
    " - creating FAISS index\n",
    " - embedding: cache/zh_camera.enc 1259 examples of dim 1024\n",
    " - creating FAISS index\n",
    " - embedding: cache/zh_mobile.enc 1333 examples of dim 1024\n",
    " - creating FAISS index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Because dataset of semeval is not exact translation of each other , some of the above results are not good. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating multi label classification task using LASER sentence embedding. \n",
    "We can have 6 aspect categories , present for each review. We will train a simple 1 layer Neural Network model using 1024 dimensional sentence embedding as input and 6 categories as output.  \n",
    "Train the model on 1700 English sentences and Validate on 1300 Dutch sentences . We are getting around 85 % accuracy and 57% f1 score(Macro) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_aspects , val_aspects, train_df , val_df = train_test_split(eng_single_aspects , data_en , test_size = 0.2 , random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb  = MultiLabelBinarizer()\n",
    "tr_eng = train_aspects['polarities'].apply(lambda x: 1 if 'positive' in x else 0).values   #mlb.fit_transform(train_aspects.polarities)\n",
    "val_eng =  val_aspects['polarities'].apply(lambda x: 1 if 'positive' in x else 0).values#mlb.transform(val_aspects.polarities)\n",
    "zh_camera  =  zu_camera_df['polarities'].apply(lambda x: 1 if 'positive' in x else 0).values  #mlb.transform(zu_camera_df.polarities)\n",
    "zh_mobile = zu_mobile_df['polarities'].apply(lambda x: 1 if 'positive' in x else 0).values #mlb.transform(zu_mobile_df.polarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(257,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_eng.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_aspects.reset_index(inplace=True , drop= True)\n",
    "# train_fn = pd.merge(train_aspects , pd.DataFrame(tr_eng , columns=mlb.classes_) , left_index=True , right_index=True)\n",
    "\n",
    "# val_aspects.reset_index(inplace=True , drop= True)\n",
    "# val_fn = pd.merge(val_aspects , pd.DataFrame(val_eng , columns=mlb.classes_) , left_index=True , right_index=True)\n",
    "\n",
    "# train_fn.drop(columns=['aspects2'] , inplace=True)\n",
    "# val_fn.drop(columns=['aspects2'] , inplace=True)\n",
    "\n",
    "# train_fn.to_csv('resturant_train_eng.csv' , index = False)\n",
    "# val_fn.to_csv('resturant_val_eng.csv' , index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scale = StandardScaler().fit(train_df)\n",
    "train_std = std_scale.transform(train_df) \n",
    "val_std = std_scale.transform(val_df)\n",
    "test1_std = std_scale.transform(data_zh_cam)\n",
    "test2_std = std_scale.transform(data_zh_mob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1027, 1024), (257, 1024), (1259, 1024), (1333, 1024))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_std.shape , val_std.shape , test1_std.shape , test2_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1027, 1]) torch.Size([257, 1]) torch.Size([1259, 1]) torch.Size([1333, 1])\n",
      "torch.Size([1027, 1024]) torch.Size([257, 1024]) torch.Size([1259, 1024]) torch.Size([1333, 1024])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "x_train,y_train,x_valid,y_valid , x_test , y_test  , x_test2 , y_test2 = map(torch.FloatTensor, (train_std,tr_eng,  val_std ,\\\n",
    "                                                                            val_eng, test1_std,zh_camera, \\\n",
    "                                                                           test2_std ,zh_mobile ))\n",
    "n,c = x_train.shape\n",
    "y_train = y_train.type(torch.FloatTensor).view(-1 , 1)\n",
    "y_valid = y_valid.type(torch.FloatTensor).view(-1 , 1)\n",
    "y_test = y_test.type(torch.FloatTensor).view(-1 , 1)\n",
    "y_test2 = y_test2.type(torch.FloatTensor).view(-1 , 1)\n",
    "\n",
    "print(y_train.shape , y_valid.shape , y_test.shape , y_test2.shape)\n",
    "print(x_train.shape , x_valid.shape , x_test.shape , x_test2.shape)\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self , p):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(1024, 64)\n",
    "#         self.hidden2 = nn.Linear(512 , 256)\n",
    "#         self.hidden3 =  nn.Linear(256 , 128)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.fc = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.dropout(self.hidden(x)))\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_ds = TensorDataset(x_train, y_train)\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size , shuffle=True)\n",
    "\n",
    "valid_ds = TensorDataset(x_valid, y_valid )\n",
    "valid_dl = DataLoader(valid_ds , batch_size= batch_size)\n",
    "\n",
    "test_ds = TensorDataset(x_test , y_test)\n",
    "test_dl = DataLoader(test_ds , batch_size=batch_size)\n",
    "\n",
    "test_ds2 = TensorDataset(x_test2 , y_test2)\n",
    "test_dl2 = DataLoader(test_ds2 , batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WrappedDataLoader():\n",
    "    def __init__(self, dl, func):\n",
    "        self.dl = dl\n",
    "        self.func = func\n",
    "        \n",
    "    def __len__(self): return len(self.dl)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        batches = iter(self.dl)\n",
    "        for b in batches: yield(self.func(*b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "def preprocess(x,y): return x.to(dev),y.to(dev)\n",
    "\n",
    "train_dl = WrappedDataLoader(train_dl, preprocess)\n",
    "valid_dl = WrappedDataLoader(valid_dl, preprocess)\n",
    "test_dl = WrappedDataLoader(test_dl , preprocess)\n",
    "test_dl2 = WrappedDataLoader(test_dl2 , preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.mean() #/ (len(correct))\n",
    "    return acc\n",
    "\n",
    "\n",
    "def fbeta_score(y_true, y_pred, beta, threshold, eps=1e-9):\n",
    "    beta2 = beta**2\n",
    "\n",
    "    y_pred = torch.ge(torch.sigmoid(y_pred).float(), threshold).float()\n",
    "    y_true = y_true.float()\n",
    "\n",
    "    true_positive = (y_pred * y_true).sum(dim=0)\n",
    "    precision = true_positive.div(y_pred.sum(dim=0).add(eps))\n",
    "    recall = true_positive.div(y_true.sum(dim=0).add(eps))\n",
    "\n",
    "    return torch.mean(\n",
    "        (precision*recall).\n",
    "        div(precision.mul(beta2) + recall + eps).\n",
    "        mul(1 + beta2))\n",
    "\n",
    "\n",
    "def f1_score(y_pred,y_true, threshold=0.5):\n",
    "    f1 = fbeta_score(y_true, y_pred, 1, threshold) #; print('f1 score' , f1)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0  \n",
    "    epoch_f1 = 0\n",
    "    model.train()\n",
    "    ct = 0\n",
    "    for x, y in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(x)\n",
    "        loss = criterion(predictions, y)\n",
    "        acc = binary_accuracy(predictions, y)\n",
    "        f1 = f1_score(predictions , y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_f1 += f1.item()  \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator) , epoch_f1/len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    epoch_f1 = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "\n",
    "            predictions = model(x)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1_score(predictions , y).item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc /len(iterator) , epoch_f1/len(iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply grid search on LR , Weight Decay , Dropout parameters , save the parameters with best f1-score on validatation data ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training results 0.7027108739404118 0.6617647058823529 0.5423290352610981\n",
      "validation results 0.4461862802505493 0.78125 0.5632232546806335\n",
      "Parameters:  0.2 0.1 0.001\n",
      "training results 0.7285676773856667 0.6237745109726401 0.5653307490489062\n",
      "validation results 0.44293752014636995 0.7875 0.5748942613601684\n",
      "Parameters:  0.3 0.1 0.001\n",
      "training results 0.674666440443081 0.6654411764705882 0.6313348289798287\n",
      "validation results 0.4190623998641968 0.825 0.6268226027488708\n",
      "Parameters:  0.3 0.05 0.001\n",
      "training results 0.6988282641943764 0.7414215697961695 0.6943592902492074\n",
      "validation results 0.372722814977169 0.85625 0.6613321185112\n",
      "Parameters:  0.4 0.05 0.01\n"
     ]
    }
   ],
   "source": [
    "best_valid_f1 = -float('inf')\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "loss_func = loss_func.to(dev)\n",
    "for drp in [0.2, 0.3,0.4,0.5,0.6]:\n",
    "    for wd in [0.1 , 0.05 , 0.01 , 0.005 , 0.001]:\n",
    "        for learning_rate in [1e-2 , 5e-3 , 1e-3]:\n",
    "            model = Model(drp); model.apply(init_weights)\n",
    "            model = model.to(dev)\n",
    "            optimizer = optim.Adam(model.parameters() , lr = learning_rate, weight_decay=wd) #[a+'_pred' for a in aspects]\n",
    "            model = model.to(dev)\n",
    "            epochs = 10\n",
    "            for epoch in range(1, epochs + 1):\n",
    "                train_loss , train_acc , train_f1 = train_model(model, train_dl, optimizer, loss_func)\n",
    "                valid_loss , valid_acc , valid_f1  = validate_model(model, valid_dl, loss_func)\n",
    "                if (valid_f1 > best_valid_f1)  & (abs(train_f1- valid_f1) <= 0.05):\n",
    "                    print('training results' , train_loss , train_acc , train_f1)\n",
    "                    print('validation results' ,valid_loss , valid_acc , valid_f1 )\n",
    "                    best_valid_f1 = valid_f1\n",
    "                    #print(f'Vaidation and train f1 score {best_valid_f1} {train_f1}')\n",
    "                    #print('accuracy validation and train',valid_acc , train_acc)\n",
    "                    print(\"Parameters: \" , drp ,  wd , learning_rate )\n",
    "                    checkpoint = {'model' : Model(drp) , 'state_dict': model.state_dict() , 'optimizer': optimizer.state_dict() }\n",
    "                    #torch.save(model.state_dict(), 'chinese_embeddings_v1.pt')\n",
    "                    torch.save(checkpoint ,'checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6686540007591247, 0.9574727696530959)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_f1 , train_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_func = nn.BCEWithLogitsLoss()\n",
    "# loss_func = loss_func.to(dev)\n",
    "# model = Model(drp); model.apply(init_weights)\n",
    "# model = model.to(dev)\n",
    "# optimizer = optim.Adam(model.parameters() , lr = learning_rate, weight_decay=wd) #[a+'_pred' for a in aspects]\n",
    "# model = model.to(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Model(0.3)\n",
    "# model.load_state_dict(torch.load('chinese_embeddings_v1.pt'))\n",
    "# #model.eval()\n",
    "\n",
    "def load_checkpoint(path):\n",
    "    checkpoint = torch.load(path)\n",
    "    model  = checkpoint['model']\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    for parameter in model.parameters():\n",
    "        parameter.requires_grad = False\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = load_checkpoint('checkpoint.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (hidden): Linear(in_features=1024, out_features=64, bias=True)\n",
       "  (activation): ReLU()\n",
       "  (dropout): Dropout(p=0.4)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.372722814977169, 0.85625, 0.6613321185112)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_model(model3, valid_dl, loss_func)\n",
    "def validate_model(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0 \n",
    "    epoch_f1 = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for x ,y  in iterator:\n",
    "\n",
    "            predictions = model(x)#.squeeze(1)\n",
    "            loss = criterion(predictions,y)\n",
    "            acc = binary_accuracy(predictions, y)\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_f1 += f1_score(predictions , y).item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc /len(iterator) , epoch_f1/len(iterator) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1024]) torch.Size([64, 1])\n",
      "torch.Size([64, 1024]) torch.Size([64, 1])\n",
      "torch.Size([64, 1024]) torch.Size([64, 1])\n",
      "torch.Size([64, 1024]) torch.Size([64, 1])\n",
      "torch.Size([1, 1024]) torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x , y in valid_dl:\n",
    "    print(x.size() , y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = model3.to(dev)\n",
    "test_preds = []\n",
    "true_label = []\n",
    "probs = []\n",
    "model3.eval()\n",
    "with torch.no_grad():\n",
    "    for x ,y  in valid_dl:\n",
    "        predictions = model3(x)\n",
    "        probs.append(torch.sigmoid(predictions).cpu().numpy())\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  \n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "# print(np.mean(np.vstack(test_preds)==np.vstack(true_label)))\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.hist(np.vstack(torch.sigmoid(predictions).cpu().numpy()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8210116731517509"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(y_valid.data.cpu().numpy()==np.vstack(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(np.vstack(probs));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.823076923076923, 0.8210116731517509)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_valid.data.cpu().numpy() ,np.vstack(test_preds)) , accuracy_score(y_valid.data.cpu().numpy() ,np.vstack(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score camera 0.714590747330961\n",
      "accuracy score camera 0.6814932486100079\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl:\n",
    "        predictions = model3(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "print('f1 score camera',f1_score(y_test.data.cpu().numpy() ,np.vstack(test_preds)) )\n",
    "print('accuracy score camera',accuracy_score(y_test.data.cpu().numpy() ,np.vstack(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1333, 1])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score mobile 0.7232472324723248\n",
      "accuracy score mobile 0.7186796699174793\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "probs = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl2:\n",
    "        predictions = model3(x)#.squeeze(1)\n",
    "        probs.append(torch.sigmoid(predictions).cpu().numpy())\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "print('f1 score mobile',f1_score(y_test2.data.cpu().numpy() ,np.vstack(test_preds)) )\n",
    "print('accuracy score mobile',accuracy_score(y_test2.data.cpu().numpy() ,np.vstack(test_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[468, 107],\n",
       "       [268, 490]])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test2.data.cpu().numpy() ,np.vstack(test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.7018094546534963\n",
      "Accuracy score 0.7073073868149324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:10: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:11: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "camera_pred = pd.DataFrame(np.vstack(test_preds) ,index=zu_camera_df.index , columns= [a+'_pred' for a in aspects])\n",
    "camera_pred2 = pd.merge(zu_camera_df, camera_pred , left_index=True ,right_index = True)\n",
    "\n",
    "print(\"F1 score\",f1_score( zh_camera, camera_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(zh_camera == camera_pred2[[a+'_pred' for a in aspects]].as_matrix()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score 0.765655739367348\n",
      "Accuracy score 0.7663165791447862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:21: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "/home/FRACTAL/swati.tiwari/anaconda3/envs/fastai38/lib/python3.7/site-packages/ipykernel_launcher.py:22: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "test_preds = []\n",
    "true_label = []\n",
    "with torch.no_grad():\n",
    "    for x ,y  in test_dl2:\n",
    "        predictions = model(x)#.squeeze(1)\n",
    "        rounded_preds = torch.round(torch.sigmoid(predictions))  #torch.round\n",
    "        preds = rounded_preds.data.cpu().numpy()\n",
    "        test_preds.append(preds)\n",
    "        true_label.append(y.data.cpu().numpy())\n",
    "        \n",
    "        \n",
    "aspects = mlb.classes_.tolist()\n",
    "\"\"\"\n",
    "Merging prediction value with original test data and observe the metrics on overall level\n",
    "\"\"\"\n",
    "mobile_pred = pd.DataFrame(np.vstack(test_preds) ,index=zu_mobile_df.index , columns= [a+'_pred' for a in aspects])\n",
    "mobile_pred2 = pd.merge(zu_mobile_df, mobile_pred , left_index=True ,right_index = True)\n",
    "\n",
    "from sklearn.metrics import f1_score , confusion_matrix , accuracy_score , precision_score , recall_score , roc_auc_score\n",
    "\n",
    "print(\"F1 score\",f1_score(zh_mobile , mobile_pred2[[a+'_pred' for a in aspects]].as_matrix() , average='macro' ))\n",
    "print(\"Accuracy score\" , np.mean(zh_mobile == mobile_pred2[[a+'_pred' for a in aspects]].as_matrix()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
